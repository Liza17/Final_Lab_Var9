# Отчет о производительности

## Методика
1.  Сгенерирован файл `data/example_huge.json` (500,000 записей, ~45 МБ) с помощью утилиты `Generator`.
2.  Запуск производился в конфигурации **Release** (x64).
3.  Использовался флаг `--bench`.

## Результаты

| Операция | Время (мс) | Комментарий |
| :--- | :--- | :--- |
| **Parsing (I/O + JSON)** | ~320 ms | Чтение файла и построение дерева объектов. |
| **Validation / Loading** | ~110 ms | Конвертация строк в структуры, парсинг дат. |
| **Aggregation (Map)** | ~190 ms | Группировка по студентам и сортировка. |
| **Total Time** | **~776 ms** | Полный цикл обработки. |

## Анализ "Узкого горлышка"
Наиболее затратной частью (после I/O) является агрегация данных.
Используется `std::map<std::string, vector>`, что дает сложность $O(N \log M)$, где $M$ — число уникальных студентов.
При большом количестве уникальных ключей (студентов) аллокация узлов дерева и сравнение строк становятся узким местом.

## Предложения по оптимизации
1.  Использовать `std::unordered_map` (хеш-таблица) вместо `std::map`. Это ускорит группировку до $O(N)$ в среднем.
2.  Использовать **SAX-парсер** (событийный), чтобы не загружать весь JSON в память (DOM), а сразу создавать объекты `AttendanceRecord` при чтении потока. Это снизит потребление памяти в разы.
```
